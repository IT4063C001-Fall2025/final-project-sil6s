{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üöç NYC Bus Reliability vs Weather, Traffic, and Service Alerts\n",
    "\n",
    "![Banner](./assets/banner.jpeg)\n",
    "\n",
    "**Goal**: Quantify how weather conditions, traffic congestion, and service disruptions affect **NYC bus reliability**, measured via **Wait Assessment (WA)** ‚Äî the share of observed trips that meet scheduled headways.\n",
    "\n",
    "This notebook fulfills **Checkpoint 2: Exploratory Data Analysis & Visualization** for IT4063C Data Technologies Analytics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß≠ Project Overview\n",
    "NYC buses operate on city streets, making them vulnerable to **weather**, **congestion**, and **service alerts**. Reliable service is critical for millions of daily riders. This analysis explores how environmental and operational factors influence reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚ùì Research Question\n",
    "**How do precipitation, snowfall, temperature, traffic speeds, and MTA bus alerts relate to monthly NYC bus Wait Assessment (2020‚Äì2024)?**\n",
    "\n",
    "**Hypothesis:** Heavy rain, snow, and traffic congestion reduce Wait Assessment (lower reliability). Months with higher alert volumes (detours, delays) will also show lower WA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üóÇÔ∏è Data Sources\n",
    "- **MTA Bus Performance (Wait Assessment)** ‚Äî `data/bus_data.csv`\n",
    "- **NOAA GHCN-Daily ‚Äì Central Park (USW00094728)** ‚Äî `data/weather_data.csv`\n",
    "- **NYC DOT Traffic API (JSON)** ‚Äî https://data.cityofnewyork.us/resource/i4gi-tjb9.json\n",
    "- **MTA Bus Alerts Feed (Protocol Buffers)** ‚Äî https://api-endpoint.mta.info/Dataservice/mtagtfsfeeds/camsys%2Fbus-alerts\n",
    "\n",
    "These sources represent **three acquisition methods**: CSV, JSON API, and Protocol Buffers. See `data_types.md` for detailed schema definitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, json, math, textwrap, time, warnings\n",
    "from datetime import datetime, timedelta\n",
    "\n", 
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import requests\n",
    "\n",
    "from google.transit import gtfs_realtime_pb2\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', 200)\n",
    "sns.set(style='whitegrid')\n",
    "\n",
    "print(\"[INFO] Notebook start at:\", datetime.now().strftime('%Y-%m-%d %H:%M:%S'))\n",
    "print(\"[INFO] Python:\", sys.version)\n",
    "print(\"[INFO] Pandas:\", pd.__version__)\n",
    "print(\"[INFO] Using working directory:\", os.getcwd())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚öôÔ∏è Configuration & Utility Helpers\n",
    "Config lives here to centralize retry/backoff, URLs, and required environment variables. The helper functions add explicit debug prints and hard assertions so failures are self-explanatory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Backoff:\n",
    "    @staticmethod\n",
    "    def sleep(i):\n",
    "        # Exponential backoff with jitter\n",
    "        delay = min(2 ** i + np.random.rand(), 10)\n",
    "        print(f\"[DEBUG] Backoff sleeping {delay:.2f}s before retry {i+1}\")\n",
    "        time.sleep(delay)\n",
    "\n",
    "CFG = {\n",
    "    'traffic_url': 'https://data.cityofnewyork.us/resource/i4gi-tjb9.json',\n",
    "    'mta_alerts_url': 'https://api-endpoint.mta.info/Dataservice/mtagtfsfeeds/camsys%2Fbus-alerts',\n",
    "    'months_back_for_traffic': 24,\n",
    "    'csv_bus': 'data/bus_data.csv',\n",
    "    'csv_weather': 'data/weather_data.csv'\n",
    "}\n",
    "\n",
    "def require_file(path):\n",
    "    assert os.path.exists(path), f\"[FATAL] Required file missing: {path}. Place it in the repo (see Data Sources).\"\n",
    "    print(f\"[OK] Found file: {path}\")\n",
    "\n",
    "def require_env(varname, optional=False):\n",
    "    val = os.getenv(varname)\n",
    "    if not val:\n",
    "        msg = f\"[{'WARN' if optional else 'FATAL'}] Env var {varname} is {'recommended' if optional else 'required'}\"\n",
    "        if optional:\n",
    "            print(msg)\n",
    "        else:\n",
    "            raise EnvironmentError(msg)\n",
    "    else:\n",
    "        print(f\"[OK] Env var {varname} present ({'optional' if optional else 'required'})\")\n",
    "    return val\n",
    "\n",
    "def nonempty_df(df, name=\"<df>\"):\n",
    "    assert isinstance(df, pd.DataFrame), f\"[FATAL] {name} is not a DataFrame\"\n",
    "    assert len(df) > 0, f\"[FATAL] {name} is empty\"\n",
    "    print(f\"[OK] {name}: shape={df.shape}\")\n",
    "    return df\n",
    "\n",
    "def must_have_cols(df, cols, name=\"<df>\"):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    assert not missing, f\"[FATAL] {name} is missing columns: {missing}\\nAvailable: {list(df.columns)}\"\n",
    "    print(f\"[OK] {name} has required columns: {cols}\")\n",
    "    return df\n",
    "\n",
    "def print_range(df, col, name):\n",
    "    if col in df:\n",
    "        m, M = df[col].min(), df[col].max()\n",
    "        print(f\"[DEBUG] {name}.{col} range: {m} ‚Üí {M}\")\n",
    "    else:\n",
    "        print(f\"[WARN] {name} missing column for range: {col}\")\n",
    "\n",
    "def pct(n, d):\n",
    "    return (n / d) * 100 if d else float('nan')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üì• Load & Normalize Datasets\n",
    "Three distinct ingestion methods:\n",
    "1) **CSV**: Bus performance, Weather\n",
    "2) **JSON API**: NYC DOT traffic\n",
    "3) **Protocol Buffers**: MTA bus alerts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- CSV 1: MTA Bus Performance ---\n",
    "require_file(CFG['csv_bus'])\n",
    "bus = pd.read_csv(CFG['csv_bus'], na_values=['', ' ', 'null', 'NULL'])\n",
    "nonempty_df(bus, 'bus_raw')\n",
    "bus.columns = [c.strip().lower().replace(' ', '_') for c in bus.columns]\n",
    "must_have_cols(bus, ['month','wait_assessment','number_of_trips_passing_wait','number_of_scheduled_trips'], 'bus_raw')\n",
    "\n",
    "bus['date'] = pd.to_datetime(bus['month'], errors='coerce')\n",
    "bad_dates = bus['date'].isna().sum()\n",
    "print(f\"[DEBUG] bus_raw bad date rows: {bad_dates} / {len(bus)} ({pct(bad_dates, len(bus)):.2f}%)\")\n",
    "bus = bus.dropna(subset=['date'])\n",
    "for col in ['number_of_trips_passing_wait','number_of_scheduled_trips']:\n",
    "    bus[col] = pd.to_numeric(bus[col].astype(str).str.replace(',', '', regex=False), errors='coerce')\n",
    "    na_cnt = bus[col].isna().sum()\n",
    "    print(f\"[DEBUG] bus_raw {col} NaN after numeric coercion: {na_cnt}\")\n",
    "bus['wait_assessment'] = pd.to_numeric(\n",
    "    bus['wait_assessment'].astype(str).str.rstrip('%'), errors='coerce') / 100.0\n",
    "print_range(bus, 'wait_assessment', 'bus_raw')\n",
    "\n",
    "# --- CSV 2: NOAA Weather ---\n",
    "require_file(CFG['csv_weather'])\n",
    "weather = pd.read_csv(CFG['csv_weather'])\n",
    "nonempty_df(weather, 'weather_raw')\n",
    "weather.columns = [c.strip().lower() for c in weather.columns]\n",
    "must_have_cols(weather, ['date'], 'weather_raw')\n",
    "weather['date'] = pd.to_datetime(weather['date'], errors='coerce')\n",
    "weather = weather.dropna(subset=['date'])\n",
    "\n",
    "num_cols = ['prcp','snow','snwd','tmax','tmin','tavg','awnd']\n",
    "for col in num_cols:\n",
    "    if col in weather:\n",
    "        weather[col] = pd.to_numeric(weather[col], errors='coerce')\n",
    "        print(f\"[DEBUG] weather_raw {col} NaN: {weather[col].isna().sum()}\")\n",
    "    else:\n",
    "        print(f\"[WARN] weather_raw missing optional column: {col}\")\n",
    "\n",
    "# Convert tenths-based NOAA values if present\n",
    "if 'tmax' in weather: weather['tmax'] = weather['tmax'] / 10.0\n",
    "if 'tmin' in weather: weather['tmin'] = weather['tmin'] / 10.0\n",
    "if 'tavg' in weather: weather['tavg'] = weather['tavg'] / 10.0\n",
    "if 'prcp' in weather: weather['prcp_mm'] = weather['prcp'] / 10.0\n",
    "if 'snow' in weather: weather['snow_mm'] = weather['snow'] / 10.0\n",
    "if 'awnd' in weather: weather['awnd_ms'] = weather['awnd'] / 10.0\n",
    "\n",
    "# Fill missing TAVG from min/max if possible\n",
    "if all(c in weather for c in ['tavg','tmin','tmax']):\n",
    "    mask = weather['tavg'].isna()\n",
    "    filled = mask.sum()\n",
    "    weather.loc[mask, 'tavg'] = (weather.loc[mask, 'tmin'] + weather.loc[mask, 'tmax']) / 2\n",
    "    print(f\"[DEBUG] weather_raw tavg filled from tmin/tmax: {filled}\")\n",
    "\n",
    "print_range(weather, 'tavg', 'weather_raw')\n",
    "print_range(weather, 'prcp_mm', 'weather_raw')\n",
    "print_range(weather, 'snow_mm', 'weather_raw')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- JSON API: NYC DOT Traffic Speeds ---\n",
    "def fetch_traffic_json(url, months_back=24, app_token=None, verify=True):\n",
    "    start_dt = (datetime.utcnow() - timedelta(days=30*months_back)).strftime('%Y-%m-%dT00:00:00.000')\n",
    "    params = {\n",
    "        '$where': f\"recordedtimestamp >= '{start_dt}'\",\n",
    "        '$limit': 50000,\n",
    "        '$order': 'recordedtimestamp DESC'\n",
    "    }\n",
    "    headers = {}\n",
    "    if app_token:\n",
    "        headers['X-App-Token'] = app_token\n",
    "\n",
    "    last_err = None\n",
    "    for i in range(4):\n",
    "        try:\n",
    "            print(f\"[INFO] Fetching traffic JSON (attempt {i+1}) from {url}\")\n",
    "            r = requests.get(url, params=params, headers=headers, timeout=25, verify=verify)\n",
    "            if r.status_code != 200:\n",
    "                last_err = RuntimeError(f\"HTTP {r.status_code}: {r.text[:200]}\")\n",
    "                print(f\"[WARN] Traffic attempt {i+1} failed ‚Üí {last_err}\")\n",
    "                Backoff.sleep(i); continue\n",
    "            data = r.json()\n",
    "            if not isinstance(data, list) or not data:\n",
    "                last_err = RuntimeError(\"Traffic API returned empty or non-list JSON\")\n",
    "                print(f\"[WARN] Traffic attempt {i+1} returned empty data\")\n",
    "                Backoff.sleep(i); continue\n",
    "            print(f\"[OK] Traffic records fetched: {len(data)}\")\n",
    "            return pd.DataFrame(data)\n",
    "        except requests.exceptions.SSLError as e:\n",
    "            raise  # fail fast so the trust store/CA issue can be fixed explicitly\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            print(f\"[WARN] Traffic attempt {i+1} error ‚Üí {e}\")\n",
    "            Backoff.sleep(i)\n",
    "    raise last_err\n",
    "\n",
    "socrata_token = os.getenv('SODA_APP_TOKEN')  # optional\n",    
    "traffic_raw = fetch_traffic_json(\n",
    "    CFG['traffic_url'],\n",
    "    months_back=CFG['months_back_for_traffic'],\n",
    "    app_token=socrata_token,\n",
    "    verify=True,\n",
    ")\n",
    "nonempty_df(traffic_raw, 'traffic_raw')\n",
    "\n",
    "# Normalize and sanity check\n",
    "ts_col = 'recordedtimestamp' if 'recordedtimestamp' in traffic_raw.columns else 'data_as_of'\n",
    "must_have_cols(traffic_raw, [ts_col, 'speed'], 'traffic_raw')\n",
    "traffic_raw['recordedtimestamp'] = pd.to_datetime(traffic_raw[ts_col], errors='coerce')\n",
    "bad_ts = traffic_raw['recordedtimestamp'].isna().sum()\n",
    "print(f\"[DEBUG] traffic_raw bad timestamps: {bad_ts} / {len(traffic_raw)}\")\n",
    "traffic_raw = traffic_raw.dropna(subset=['recordedtimestamp'])\n",
    "traffic_raw['speed'] = pd.to_numeric(traffic_raw['speed'], errors='coerce')\n",
    "speed_nan = traffic_raw['speed'].isna().sum()\n",
    "print(f\"[DEBUG] traffic_raw speed NaN after coercion: {speed_nan}\")\n",
    "traffic_raw = traffic_raw.dropna(subset=['speed'])\n",
    "print_range(traffic_raw, 'speed', 'traffic_raw')\n",
    "\n",
    "traffic_raw['date'] = traffic_raw['recordedtimestamp'].dt.to_period('M').dt.to_timestamp()\n",
    "traffic_monthly = (traffic_raw\n",
    "    .groupby('date', as_index=False)['speed']\n",
    "    .mean()\n",
    "    .rename(columns={'speed':'mean_speed_mph'}))\n",
    "nonempty_df(traffic_monthly, 'traffic_monthly')\n",
    "print_range(traffic_monthly, 'mean_speed_mph', 'traffic_monthly')\n",
    "print(f\"[INFO] traffic_monthly rows: {len(traffic_monthly)}; date span: {traffic_monthly['date'].min()} ‚Üí {traffic_monthly['date'].max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# --- Protocol Buffers: MTA Bus Alerts ---\n",
    "def fetch_mta_alerts_feed(url, api_key):\n",
    "    headers = {'x-api-key': api_key}\n",
    "    for i in range(4):\n",
    "        try:\n", 
    "            print(f\"[INFO] Fetching MTA GTFS-rt alerts (attempt {i+1})\")\n",
    "            r = requests.get(url, headers=headers, timeout=25)\n",
    "            if r.status_code != 200:\n",
    "                print(f\"[WARN] Alerts attempt {i+1} HTTP {r.status_code}: {r.text[:160]}\")\n",
    "                Backoff.sleep(i); continue\n",
    "            feed = gtfs_realtime_pb2.FeedMessage()\n",
    "            feed.ParseFromString(r.content)\n",
    "            print(f\"[OK] Alerts entities fetched: {len(feed.entity)}\")\n",
    "            return feed\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] Alerts attempt {i+1} error ‚Üí {e}\")\n",
    "            Backoff.sleep(i)\n",
    "    raise RuntimeError(\"Failed to fetch/parse MTA GTFS-rt alerts after retries\")\n",
    "\n",
    "mta_api_key = require_env('MTA_API_KEY', optional=False)\n",
    "alerts_feed = fetch_mta_alerts_feed(CFG['mta_alerts_url'], mta_api_key)\n",
    "header_ts = pd.to_datetime(datetime.utcfromtimestamp(alerts_feed.header.timestamp), utc=True).tz_convert('America/New_York') if alerts_feed.header.timestamp else pd.Timestamp.utcnow()\n",
    "alerts = [e for e in alerts_feed.entity if e.HasField('alert')]\n",
    "alerts_df = pd.DataFrame([\n",
    "    {\n",
    "        'date': header_ts.normalize(),\n",
    "        'alert_count': len(alerts)\n",
    "    }\n",
    "])\n",
    "nonempty_df(alerts_df, 'alerts_df')\n",
    "print(alerts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Exploratory Data Analysis (EDA)\n",
    "Quick statistical summaries, distribution checks, correlations, and basic data health scans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('--- BUS DATA (raw) ---')\n",
    "display(bus.describe(include='all'))\n",
    "print('\\n--- WEATHER DATA (raw) ---')\n",
    "display(weather.describe(include='all'))\n",
    "print('\\n--- TRAFFIC DATA (monthly) ---')\n",
    "display(traffic_monthly.describe(include='all'))\n",
    "print('\\n--- ALERTS SNAPSHOT ---')\n",
    "display(alerts_df)\n",
    "\n",
    "# Types & null scans\n",
    "def scan_df(df, name):\n",
    "    print(f\"\\n[SCAN] {name} dtypes:\")\n",
    "    print(df.dtypes)\n",
    "    nulls = df.isna().sum().sort_values(ascending=False)\n",
    "    print(f\"[SCAN] {name} null counts (top 10):\\n{nulls.head(10)}\")\n",
    "    print(f\"[SCAN] {name} duplicated rows: {df.duplicated().sum()}\")\n",
    "\n",
    "scan_df(bus, 'bus')\n",
    "scan_df(weather, 'weather')\n",
    "scan_df(traffic_monthly, 'traffic_monthly')\n",
    "\n",
    "# Identify obvious issues\n",
    "print(\"\\n[CHECK] Negative or zero scheduled trips in bus data:\")\n",
    "if 'number_of_scheduled_trips' in bus:\n",
    "    print(bus.loc[bus['number_of_scheduled_trips'] <= 0, ['date','number_of_scheduled_trips']].head())\n",
    "else:\n",
    "    print('[WARN] number_of_scheduled_trips column missing after normalization')\n",
    "\n",
    "print(\"\\n[CHECK] Out-of-range WA values (should be 0‚Äì1):\")\n",
    "if 'wait_assessment' in bus:\n",
    "    print(bus.loc[(bus['wait_assessment'] < 0) | (bus['wait_assessment'] > 1), ['date','wait_assessment']].head())\n",
    "else:\n",
    "    print('[WARN] wait_assessment column missing after normalization')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üßÆ Aggregate Monthly Metrics\n",
    "All sources are aligned at the **month** granularity for merging and comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b = bus.copy()\n",
    "b['wa_w'] = b['wait_assessment'] * b['number_of_scheduled_trips']\n",
    "bus_monthly = (b.groupby('date', as_index=False)\n",
    "    .agg(\n",
    "        wa_w=('wa_w','sum'),\n",
    "        scheduled=('number_of_scheduled_trips','sum'),\n",
    "        passing=('number_of_trips_passing_wait','sum')\n",
    "    )\n",
    "    .assign(\n",
    "        wa_weighted=lambda x: x.wa_w/x.scheduled,\n",
    "        pct_passing=lambda x: x.passing/x.scheduled\n",
    "    )\n",
    ")\n",
    "nonempty_df(bus_monthly, 'bus_monthly')\n",
    "print_range(bus_monthly, 'wa_weighted', 'bus_monthly')\n",
    "\n",
    "weather_monthly = (weather\n",
    "    .groupby(weather['date'].dt.to_period('M').dt.to_timestamp(), as_index=False)\n",
    "    .agg(\n",
    "        tavg=('tavg','mean'), tmax=('tmax','mean'), tmin=('tmin','mean'),\n",
    "        prcp_mm=('prcp_mm','sum'), snow_mm=('snow_mm','sum'), awnd_ms=('awnd_ms','mean')\n",
    "    )\n",
    "    .rename(columns={'date':'date'})\n",
    ")\n",
    "nonempty_df(weather_monthly, 'weather_monthly')\n",
    "\n",
    "merged = (bus_monthly\n",
    "    .merge(weather_monthly, on='date', how='inner')\n",
    "    .merge(traffic_monthly, on='date', how='left')\n",
    "    .merge(alerts_df, on='date', how='left')\n",
    ")\n",
    "nonempty_df(merged, 'merged')\n",
    "print(f\"[INFO] merged rows: {len(merged)}; date span: {merged['date'].min()} ‚Üí {merged['date'].max()}\")\n",
    "display(merged.head(10))\n",
    "\n",
    "# Critical sanity: WA within [0,1]\n",
    "wa_oob = merged[(merged['wa_weighted'] < 0) | (merged['wa_weighted'] > 1)]\n",
    "assert len(wa_oob) == 0, f\"[FATAL] Found {len(wa_oob)} out-of-bounds WA rows\"\n",
    "print('[OK] All WA values within [0,1] after aggregation')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Visualizations\n",
    "At least four distinct visualizations using multiple libraries. Short descriptions accompany each plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Distribution of Monthly Wait Assessment (Seaborn)\n",
    "sns.histplot(merged['wa_weighted'].dropna(), kde=True, bins=20)\n",
    "plt.title('Distribution of Monthly Wait Assessment (Weighted)')\n",
    "plt.xlabel('Wait Assessment (0‚Äì1)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.show()\n",
    "print(\"Insight: WA is generally concentrated toward higher values if the histogram skews right; heavy left-tail suggests frequent reliability issues.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Correlation Heatmap (Seaborn)\n",
    "num_cols = [c for c in ['wa_weighted','tavg','prcp_mm','snow_mm','awnd_ms','mean_speed_mph','alert_count'] if c in merged]\n",
    "corr = merged[num_cols].corr()\n",
    "sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')\n",
    "plt.title('Correlation Heatmap: Weather, Traffic, Alerts vs Reliability')\n",
    "plt.show()\n",
    "print(\"Insight: Negative correlation between precip/snow and WA suggests weather impact; positive correlation between mean traffic speed and WA suggests congestion effects.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Traffic Speed vs WA (Matplotlib)\n",
    "plt.scatter(merged['mean_speed_mph'], merged['wa_weighted'])\n",
    "plt.xlabel('Mean Traffic Speed (mph)')\n",
    "plt.ylabel('Wait Assessment (0‚Äì1)')\n",
    "plt.title('Bus Reliability vs Traffic Congestion')\n",
    "plt.grid(True, linewidth=0.4)\n",
    "plt.show()\n",
    "print(\"Insight: An upward slope indicates that higher speeds associate with better reliability; dispersion reveals noise and other drivers.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Trends: WA & Mean Traffic Speed over Time (Plotly)\n",
    "fig = px.line(\n",
    "    merged.sort_values('date'),\n",
    "    x='date', y=[c for c in ['wa_weighted','mean_speed_mph'] if c in merged],\n",
    "    title='Trends: Wait Assessment & Mean Traffic Speed'\n",
    ")\n",
    "fig.update_layout(xaxis_title='Date', yaxis_title='Value')\n",
    "fig.show()\n",
    "print(\"Insight: Co-movement over months hints at traffic-reliability coupling; decoupling suggests other factors (alerts, incidents, seasonal ops).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Precipitation vs Alert Count (Plotly, colored by WA)\n",
    "if all(c in merged for c in ['prcp_mm','alert_count','wa_weighted']):\n",
    "    fig2 = px.scatter(\n",
    "        merged, x='prcp_mm', y='alert_count', color='wa_weighted',\n",
    "        title='Precipitation vs Alert Count (Colored by WA)'\n",
    "    )\n",
    "    fig2.show()\n",
    "    print(\"Insight: Clusters at high precipitation and high alerts with lower WA would support the hypothesis that weather + disruptions degrade reliability.\")\n",
    "else:\n",
    "    print('[WARN] Missing one of prcp_mm/alert_count/wa_weighted; skipping scatter.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üßπ Data Cleaning & Transformation\n",
    "Actions reflect the issues found during EDA: outliers (precip/snow), missing values, duplicate rows, and data type coercions. The goal is a modeling-ready monthly dataset aligned across sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = merged.copy()\n",
    "\n",
    "# Drop rows lacking the target\n",
    "before = len(model_df)\n",
    "model_df = model_df.dropna(subset=['wa_weighted'])\n",
    "after = len(model_df)\n",
    "print(f\"[CLEAN] Dropped {before - after} rows without WA\")\n",
    "\n",
    "# Clip extreme precipitation/snow outliers using IQR\n",
    "for col in ['prcp_mm','snow_mm']:\n",
    "    if col in model_df:\n",
    "        q1, q3 = model_df[col].quantile([0.25,0.75])\n",
    "        iqr = q3 - q1\n",
    "        lower, upper = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "        clipped = ((model_df[col] < lower) | (model_df[col] > upper)).sum()\n",
    "        model_df[col] = model_df[col].clip(lower, upper)\n",
    "        print(f\"[CLEAN] {col} clipped outliers: {clipped} (bounds {lower:.2f}..{upper:.2f})\")\n",
    "    else:\n",
    "        print(f\"[CLEAN] {col} not present; skipping outlier clipping\")\n",
    "\n",
    "# De-duplicate monthly rows if any\n",
    "dups = model_df.duplicated(subset=['date']).sum()\n",
    "if dups:\n",
    "    print(f\"[CLEAN] Found {dups} duplicate month rows; aggregating mean over duplicates\")\n",
    "    model_df = model_df.groupby('date', as_index=False).mean(numeric_only=True)\n",
    "else:\n",
    "    print(\"[CLEAN] No duplicate monthly rows detected\")\n",
    "\n",
    "display(model_df.head())\n",
    "print_range(model_df, 'wa_weighted', 'model_df')\n",
    "\n",
    "# Final schema sanity\n",
    "required_for_model = ['wa_weighted','tavg','prcp_mm','snow_mm','awnd_ms','mean_speed_mph','alert_count']\n",
    "missing_for_model = [c for c in required_for_model if c not in model_df.columns]\n",
    "if missing_for_model:\n",
    "    print(f\"[WARN] Model features missing: {missing_for_model}. Modeling section will handle partial features.\")\n",
    "else:\n",
    "    print('[OK] All planned model features present')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Summary\n",
    "- **Missing values:** Dropped for `wa_weighted` and critical joins.\n",
    "- **Outliers:** Clipped for precipitation and snow via IQR.\n",
    "- **Duplicates:** Checked at monthly level; aggregated if present.\n",
    "- **Types:** Coerced numeric fields; standardized `date` to monthly timestamps.\n",
    "- **Sanity:** Asserted WA in [0,1]; scanned ranges and nulls for each dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üß© Machine Learning Plan (Preview)\n",
    "- **Target:** `wa_weighted`\n",
    "- **Candidate features:** `tavg`, `prcp_mm`, `snow_mm`, `awnd_ms`, `mean_speed_mph`, `alert_count`\n",
    "- **Models:** Linear Regression ‚Üí Random Forest ‚Üí Ridge/Lasso\n",
    "- **Evaluation:** MAE, RMSE, R¬≤ using a time-based split (train on early months, validate on later months).\n",
    "- **Challenges:** Limited sample size at monthly granularity; collinearity (e.g., `tavg` and seasonality); missing alert coverage for past months (alerts snapshot is current). Consider aggregating alerts over history if API access allows or proxy with incident-rich periods.\n",
    "- **Mitigations:** Regularization (Ridge/Lasso), feature scaling where appropriate, and careful backtesting windows to avoid leakage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîÑ Prior Feedback & Updates\n",
    "- Expanded from weather-only to include **traffic** (JSON API) and **service alerts** (Protocol Buffers).\n",
    "- Added robust **sanity checks**, **debug messages**, and **assertions** to make failures explicit.\n",
    "- Ensured **4+ visualizations** across **multiple libraries** (Seaborn, Matplotlib, Plotly).\n",
    "- Strengthened cleaning and integration pipeline; enforced WA bounds and typed coercions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep this as the last cell for grading automation\n",
    "!jupyter nbconvert --to python source.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IT4063C",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
