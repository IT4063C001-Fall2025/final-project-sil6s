{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🚍 NYC Bus Reliability vs Weather, Traffic, and Service Alerts\n",
    "\n",
    "![Banner](./assets/banner.jpeg)\n",
    "\n",
    "**Goal**: Quantify how weather conditions, traffic congestion, and service disruptions affect **NYC bus reliability**, measured via **Wait Assessment (WA)** — the share of observed trips meeting scheduled headways.\n",
    "\n",
    "This notebook fulfills **Checkpoint 2: Exploratory Data Analysis & Visualization** for IT4063C Data Technologies Analytics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧭 Project Overview\n",
,    "NYC buses run in mixed traffic and are exposed to weather and operational incidents. The analysis relates reliability to weather (precipitation, snow, temperature), roadway speeds, and the volume of service alerts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ❓ Research Question\n",
    "**How do precipitation, snowfall, temperature, traffic speeds, and MTA bus alerts relate to monthly NYC bus Wait Assessment (2020–2024)?**\n",
    "\n",
    "**Hypothesis:** Heavier precipitation/snow and slower traffic correlate with lower Wait Assessment; months with more alerts also show lower WA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🗂️ Data Sources\n",
    "- **MTA Bus Performance (Wait Assessment)** — local CSV → `data/bus_data.csv`\n",
    "- **NOAA GHCN-Daily – Central Park (USW00094728)** — local CSV → `data/weather_data.csv`\n",
    "- **NYC DOT Traffic Speeds** — live JSON API → `https://data.cityofnewyork.us/resource/i4gi-tjb9.json`\n",
    "- **MTA Bus Alerts** — live Protocol Buffers (GTFS-realtime) → `https://api-endpoint.mta.info/Dataservice/mtagtfsfeeds/camsys/bus-alerts`\n",
    "\n",
    "**Acquisition modes covered:** CSV file, JSON API, Protocol Buffers API.\n",
    "\n",
    "> Notes:\n",
    "> - An **MTA API key** is usually required. Set environment variable `MTA_API_KEY`.\n",
    "> - A Socrata **app token** is optional but recommended for NYC Open Data. Set `SODA_APP_TOKEN`.\n",
    "> - SSL verification is kept **on**. If a corporate proxy injects a custom CA, set `REQUESTS_CA_BUNDLE` to your CA bundle path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Setup & helpers ======================================================\n",
    "import os, sys, time, json, math, re\n",
    "from datetime import datetime, timedelta, timezone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import requests\n",
    "\n",
    "sns.set(style='whitegrid')\n",
    "pd.set_option('display.max_columns', 200)\n",
    "pd.set_option('display.width', 140)\n",
    "\n",
    "def now_ts():\n",
    "    return datetime.now().strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n", 
    "def dbg(msg):\n",
    "    print(f\"[DEBUG {now_ts()}] {msg}\")\n",
    "\n",
    "def warn(msg):\n",
    "    print(f\"[WARN  {now_ts()}] {msg}\")\n",
    "\n",
    "def err(msg):\n",
    "    print(f\"[ERROR {now_ts()}] {msg}\")\n",
    "\n",
    "class Backoff:\n",
    "    @staticmethod\n",
    "    def sleep(try_idx, base=0.75, cap=8.0):\n",
    "        delay = min(cap, base * (2 ** try_idx))\n",
    "        time.sleep(delay)\n",
    "\n",
    "def require_cols(df, cols, label):\n",
    "    missing = [c for c in cols if c not in df.columns]\n",
    "    if missing:\n",
    "        raise ValueError(f\"{label} missing required columns: {missing}\")\n",
    "    dbg(f\"{label}: required columns present → {cols}\")\n",
    "\n",
    "def missing_report(df, name):\n",
    "    m = df.isna().mean().sort_values(ascending=False)\n",
    "    dbg(f\"Missingness report for {name}:\\n\" + m.head(20).to_string())\n",
    "    return m\n",
    "\n",
    "def to_month(dt_series):\n",
    "    return pd.to_datetime(dt_series).dt.to_period('M').dt.to_timestamp()\n",
    "\n",
    "# Configuration (no silent fallbacks)\n",
    "CFG = {\n",
    "    'bus_csv': 'data/bus_data.csv',\n",
    "    'weather_csv': 'data/weather_data.csv',\n",
    "    'traffic_url': 'https://data.cityofnewyork.us/resource/i4gi-tjb9.json',\n",
    "    'mta_alerts_url': 'https://api-endpoint.mta.info/Dataservice/mtagtfsfeeds/camsys/bus-alerts',\n",
    "    'months_back_for_traffic': 24  # fetch ~2 years of speeds\n",
    "}\n",
    "dbg(f\"Config: {CFG}\")\n",
    "MTA_API_KEY = os.getenv('MTA_API_KEY')\n",
    "SODA_APP_TOKEN = os.getenv('SODA_APP_TOKEN')\n",
    "CUSTOM_CA = os.getenv('REQUESTS_CA_BUNDLE')\n",
    "if CUSTOM_CA:\n",
    "    dbg(f\"Using custom CA bundle at {CUSTOM_CA}\")\n",
    "\n",
    "# Ensure gtfs-realtime dependency is available (install if missing)\n",
    "try:\n",
    "    from google.transit import gtfs_realtime_pb2\n",
    "    GTFS_OK = True\n",
    "except Exception as e:\n",
    "    warn(f\"gtfs-realtime not installed: {e}. Attempting pip install...\")\n",
    "    import subprocess, sys as _sys\n",
    "    _ = subprocess.check_call([_sys.executable, '-m', 'pip', 'install', '--quiet', 'gtfs-realtime-bindings'])\n",
    "    from google.transit import gtfs_realtime_pb2\n",
    "    GTFS_OK = True\n",
    "dbg(\"gtfs-realtime-bindings ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📥 Load & Normalize Datasets\n",
    "Two local CSVs, one live JSON API (traffic), and one live Protocol Buffers API (MTA alerts). If a call fails, execution stops with an actionable message. No offline fallbacks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 1) Local CSV: MTA Bus Performance ===================================\n",
    "def load_bus_csv(path):\n",
    "    dbg(f\"Loading bus CSV → {path}\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Expected bus CSV at {path}\")\n",
    "    bus = pd.read_csv(path, na_values=['', ' ', 'null', 'NULL'])\n",
    "    dbg(f\"Raw bus shape: {bus.shape}\")\n",
    "    bus.columns = [c.strip().lower().replace(' ', '_') for c in bus.columns]\n",
    "\n",
    "    # Try to normalize column names\n",
    "    rename_map = {}\n",
    "    if 'month' not in bus.columns and 'date' in bus.columns: rename_map['date'] = 'month'\n",
    "    if 'number_of_trips_passing_wait' not in bus.columns and 'trips_passing_wait' in bus.columns:\n",
    "        rename_map['trips_passing_wait'] = 'number_of_trips_passing_wait'\n",
    "    if 'number_of_scheduled_trips' not in bus.columns and 'scheduled_trips' in bus.columns:\n",
    "        rename_map['scheduled_trips'] = 'number_of_scheduled_trips'\n",
    "    if rename_map:\n",
    "        dbg(f\"Renaming columns: {rename_map}\")\n",
    "        bus = bus.rename(columns=rename_map)\n",
    "\n",
    "    require_cols(bus, ['month','number_of_scheduled_trips','number_of_trips_passing_wait','wait_assessment'], 'bus')\n",
    "    bus['date'] = pd.to_datetime(bus['month'], errors='coerce')\n",
    "    for col in ['number_of_trips_passing_wait','number_of_scheduled_trips']:\n",
    "        bus[col] = pd.to_numeric(bus[col].astype(str).str.replace(',', ''), errors='coerce')\n",
    "    bus['wait_assessment'] = pd.to_numeric(bus['wait_assessment'].astype(str).str.rstrip('%'), errors='coerce')/100.0\n",
    "\n",
    "    before = len(bus)\n",
    "    bus = bus.dropna(subset=['date']).copy()\n",
    "    if len(bus) < before:\n",
    "        warn(f\"Dropped {before-len(bus)} rows with invalid dates from bus CSV\")\n",
    "\n",
    "    dbg(f\"Clean bus shape: {bus.shape}; date range: {bus['date'].min()} → {bus['date'].max()}\")\n",
    "    missing_report(bus, 'bus')\n",
    "    return bus\n",
    "\n",
    "bus = load_bus_csv(CFG['bus_csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 2) Local CSV: NOAA Weather (Central Park) ============================\n",
    "def load_weather_csv(path):\n",
    "    dbg(f\"Loading weather CSV → {path}\")\n",
    "    if not os.path.exists(path):\n",
    "        raise FileNotFoundError(f\"Expected weather CSV at {path}\")\n",
    "    header = list(pd.read_csv(path, nrows=0).columns)\n",
    "    preferred = ['STATION','NAME','DATE','PRCP','SNOW','SNWD','TMAX','TMIN','TAVG','AWND']\n",
    "    usecols = [c for c in preferred if c in header]\n",
    "    weather = pd.read_csv(path, usecols=usecols)\n",
    "    weather.columns = [c.lower() for c in weather.columns]\n",
    "\n",
    "    weather['date'] = pd.to_datetime(weather['date'], errors='coerce')\n",
    "    for c in ['prcp','snow','snwd','tmax','tmin','tavg','awnd']:\n",
    "        if c in weather: weather[c] = pd.to_numeric(weather[c], errors='coerce')\n",
    "    # NOAA tenths units\n",
    "    if 'tmax' in weather: weather['tmax'] /= 10.0\n",
    "    if 'tmin' in weather: weather['tmin'] /= 10.0\n",
    "    if 'tavg' in weather: weather['tavg'] /= 10.0\n",
    "    if 'prcp' in weather: weather['prcp_mm'] = weather['prcp']/10.0\n",
    "    if 'snow' in weather: weather['snow_mm'] = weather['snow']/10.0\n",
    "    if 'awnd' in weather: weather['awnd_ms'] = weather['awnd']/10.0\n",
    "    if all(c in weather for c in ['tmin','tmax']):\n",
    "        if 'tavg' not in weather or weather['tavg'].isna().any():\n",
    "            mask = weather.get('tavg', pd.Series(dtype=float)).isna() if 'tavg' in weather else pd.Series([True]*len(weather))\n",
    "            weather.loc[mask, 'tavg'] = (weather.loc[mask, 'tmin'] + weather.loc[mask, 'tmax'])/2\n",
    "\n",
    "    # Reasonable bounds for NYC\n",
    "    def clip(df, col, lo=None, hi=None):\n",
    "        if col not in df: return df\n",
    "        n = len(df)\n",
    "        if lo is not None:\n",
    "            below = (df[col] < lo).sum()\n",
    "            if below: warn(f\"weather.{col}: {below}/{n} < {lo}; clipping\")\n",
    "            df.loc[df[col] < lo, col] = lo\n",
    "        if hi is not None:\n",
    "            above = (df[col] > hi).sum()\n",
    "            if above: warn(f\"weather.{col}: {above}/{n} > {hi}; clipping\")\n",
    "            df.loc[df[col] > hi, col] = hi\n",
    "        return df\n",
    "    clip(weather, 'tavg', -30, 45)\n",
    "    clip(weather, 'tmin', -40, 45)\n",
    "    clip(weather, 'tmax', -30, 50)\n",
    "    clip(weather, 'prcp_mm', 0, 500)\n",
    "    clip(weather, 'snow_mm', 0, 1000)\n",
    "\n",
    "    require_cols(weather, ['date','tavg'], 'weather')\n",
    "    dbg(f\"Clean weather shape: {weather.shape}; date range: {weather['date'].min()} → {weather['date'].max()}\")\n",
    "    missing_report(weather, 'weather')\n",
    "    return weather\n",
    "\n",
    "weather = load_weather_csv(CFG['weather_csv'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 3) Live JSON API: NYC DOT Traffic Speeds =============================\n",
    "def fetch_traffic_json(url, months_back=24, app_token=None, verify=True):\n",
    "    # Build a 2-year window to keep payload reasonable and align to bus months\n",
    "    start_dt = (datetime.utcnow() - timedelta(days=30*months_back)).strftime('%Y-%m-%dT00:00:00.000')\n",
    "    params = {\n",
    "        '$where': f\"recordedtimestamp >= '{start_dt}'\",\n",
    "        '$limit': 50000,               # adjust if needed\n",
    "        '$order': 'recordedtimestamp DESC'\n",
    "    }\n",
    "    headers = {}\n",
    "    if app_token:\n",
    "        headers['X-App-Token'] = app_token\n",
    "    dbg(f\"Requesting traffic JSON with params: {params}\")\n",
    "\n",
    "    last_err = None\n",
    "    for i in range(4):\n",
    "        try:\n",
    "            r = requests.get(url, params=params, headers=headers, timeout=20, verify=verify)\n",
    "            if r.status_code != 200:\n",
    "                last_err = RuntimeError(f\"HTTP {r.status_code}: {r.text[:200]}\")\n",
    "                warn(f\"Traffic request failed (attempt {i+1}) → {last_err}\")\n",
    "                Backoff.sleep(i)\n",
    "                continue\n",
    "            data = r.json()\n",
    "            if not isinstance(data, list) or len(data) == 0:\n",
    "                last_err = RuntimeError(\"Traffic API returned empty or non-list JSON\")\n",
    "                warn(f\"Traffic empty result (attempt {i+1})\")\n",
    "                Backoff.sleep(i)\n",
    "                continue\n",
    "            dbg(f\"Traffic records fetched: {len(data)}\")\n",
    "            return pd.DataFrame(data)\n",
    "        except requests.exceptions.SSLError as e:\n",
    "            err(\"SSL error while contacting NYC Open Data. Ensure system trusts the CA chain or set REQUESTS_CA_BUNDLE.\")\n",
    "            last_err = e\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            warn(f\"Traffic request error (attempt {i+1}) → {e}\")\n",
    "            Backoff.sleep(i)\n",
    "    raise last_err\n",
    "\n",
    "traffic_raw = fetch_traffic_json(CFG['traffic_url'], months_back=CFG['months_back_for_traffic'], app_token=SODA_APP_TOKEN, verify=True)\n",
    "dbg(f\"Traffic raw shape: {traffic_raw.shape}; columns: {list(traffic_raw.columns)}\")\n",
    "\n",
    "# Normalize traffic\n",
    "def normalize_traffic(df):\n",
    "    # Common field names on i4gi-tjb9: recordedtimestamp, speed, travel_time, linkid (varies by export)\n",
    "    cols_present = list(df.columns)\n",
    "    # Harmonize timestamp column\n",
    "    ts_col = 'recordedtimestamp' if 'recordedtimestamp' in df else ('data_as_of' if 'data_as_of' in df else None)\n",
    "    if ts_col is None:\n",
    "        raise ValueError(f\"No timestamp column found in traffic payload. Columns: {cols_present}\")\n",
    "    df['recordedtimestamp'] = pd.to_datetime(df[ts_col], errors='coerce')\n",
    "    if df['recordedtimestamp'].isna().all():\n",
    "        raise ValueError(\"Traffic timestamps all invalid after parsing.\")\n",
    "    # speed often string; coerce\n", 
    "    if 'speed' not in df:\n",
    "        raise ValueError(f\"No 'speed' column found in traffic. Columns: {cols_present}\")\n",
    "    df['speed'] = pd.to_numeric(df['speed'], errors='coerce')\n",
    "    # monthly average speed\n",
    "    df['date'] = to_month(df['recordedtimestamp'])\n",
    "    monthly = df.groupby('date', as_index=False)['speed'].mean().rename(columns={'speed':'mean_speed_mph'})\n",
    "    dbg(f\"Traffic monthly rows: {len(monthly)}; date span: {monthly['date'].min()} → {monthly['date'].max()}\")\n",
    "    return df, monthly\n",
    "\n",
    "traffic, traffic_monthly = normalize_traffic(traffic_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === 4) Live Protocol Buffers API: MTA Bus Alerts =========================\n",
    "def fetch_mta_alerts(url, api_key=None, verify=True):\n",
    "    if not api_key:\n",
    "        raise EnvironmentError(\"MTA_API_KEY is required to call the MTA alerts API. Set environment variable MTA_API_KEY.\")\n",
    "    headers = {'x-api-key': api_key}\n",
    "    last_err = None\n",
    "    for i in range(4):\n",
    "        try:\n",
    "            r = requests.get(url, headers=headers, timeout=20, verify=verify)\n",
    "            if r.status_code != 200:\n",
    "                last_err = RuntimeError(f\"HTTP {r.status_code}: {r.text[:200]}\")\n",
    "                warn(f\"MTA alerts request failed (attempt {i+1}) → {last_err}\")\n",
    "                Backoff.sleep(i)\n",
    "                continue\n",
    "            return r.content\n",
    "        except requests.exceptions.SSLError as e:\n",
    "            err(\"SSL error while contacting MTA. Ensure system trusts the CA chain or set REQUESTS_CA_BUNDLE.\")\n",
    "            last_err = e\n",
    "            break\n",
    "        except Exception as e:\n",
    "            last_err = e\n",
    "            warn(f\"MTA alerts request error (attempt {i+1}) → {e}\")\n",
    "            Backoff.sleep(i)\n",
    "    raise last_err\n",
    "\n",
    "alerts_blob = fetch_mta_alerts(CFG['mta_alerts_url'], api_key=MTA_API_KEY, verify=True)\n",
    "dbg(f\"Alerts protobuf bytes: {len(alerts_blob)}\")\n",
    "\n",
    "def parse_alerts_proto(blob):\n",
    "    feed = gtfs_realtime_pb2.FeedMessage()\n",
    "    feed.ParseFromString(blob)\n",
    "    ts = pd.Timestamp.utcfromtimestamp(feed.header.timestamp).tz_convert(None) if feed.header.timestamp else pd.Timestamp(datetime.utcnow())\n",
    "    entities = [e for e in feed.entity if e.HasField('alert')]\n",
    "    # Create a one-row monthly series with the count at feed time; in practice poll & aggregate by month\n",
    "    row = {'date': ts.to_period('M').to_timestamp(), 'alert_count': len(entities)}\n",
    "    df = pd.DataFrame([row])\n",
    "    dbg(f\"Parsed {len(entities)} alerts at {ts}\")\n",
    "    return df\n",
    "\n",
    "alerts_df = parse_alerts_proto(alerts_blob)\n",
    "display(alerts_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔍 Exploratory Data Analysis (EDA)\n",
    "Statistical summaries, distributions, correlations, and data issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- BUS DATA --------------------------------------------------------------\n",
    "print('--- BUS DATA: head ---')\n",
    "display(bus.head())\n",
    "print('\\n--- BUS DATA: describe ---')\n",
    "display(bus.describe(include='all'))\n",
    "bus_dups = bus.duplicated(subset=['date']).sum()\n",
    "dbg(f\"BUS duplicates by date: {bus_dups}\")\n",
    "missing_report(bus, 'bus')\n",
    "\n",
    "# --- WEATHER DATA ----------------------------------------------------------\n",
    "print('\\n--- WEATHER DATA: head ---')\n",
    "display(weather.head())\n",
    "print('\\n--- WEATHER DATA: describe ---')\n",
    "display(weather.describe(include='all'))\n",
    "weather_dups = weather.duplicated(subset=['date']).sum()\n",
    "dbg(f\"WEATHER duplicates by daily date: {weather_dups}\")\n",
    "missing_report(weather, 'weather')\n",
    "\n",
    "# --- TRAFFIC DATA ----------------------------------------------------------\n",
    "print('\\n--- TRAFFIC RAW: head ---')\n",
    "display(traffic.head())\n",
    "print('\\n--- TRAFFIC MONTHLY: describe ---')\n",
    "display(traffic_monthly.describe(include='all'))\n",
    "missing_report(traffic_monthly, 'traffic_monthly')\n",
    "\n",
    "# --- ALERTS DATA -----------------------------------------------------------\n",
    "print('\\n--- ALERTS (monthly snapshot) ---')\n",
    "display(alerts_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧮 Aggregate Monthly Metrics\n",
    "Aggregate to monthly level and join the four sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_bus(bus):\n",
    "    b = bus.copy()\n",
    "    b['wa_w'] = b['wait_assessment'] * b['number_of_scheduled_trips']\n",
    "    monthly = b.groupby('date', as_index=False).agg(\n",
    "        wa_w=('wa_w','sum'),\n",
    "        scheduled=('number_of_scheduled_trips','sum'),\n",
    "        passing=('number_of_trips_passing_wait','sum')\n",
    "    )\n",
    "    monthly['wa_weighted'] = np.where(monthly['scheduled']>0, monthly['wa_w']/monthly['scheduled'], np.nan)\n",
    "    monthly['pct_passing'] = np.where(monthly['scheduled']>0, monthly['passing']/monthly['scheduled'], np.nan)\n",
    "    dbg(f\"Bus monthly rows: {len(monthly)}; null wa_weighted: {monthly['wa_weighted'].isna().sum()}\")\n",
    "    return monthly\n",
    "\n",
    "def aggregate_weather(weather):\n",
    "    w = weather.copy()\n",
    "    w['month'] = to_month(w['date'])\n",
    "    agg = w.groupby('month', as_index=False).agg(\n",
    "        tavg=('tavg','mean'), tmax=('tmax','mean'), tmin=('tmin','mean'),\n",
    "        prcp_mm=('prcp_mm','sum') if 'prcp_mm' in w else ('tavg','size'),\n",
    "        snow_mm=('snow_mm','sum') if 'snow_mm' in w else ('tavg','size'),\n",
    "        awnd_ms=('awnd_ms','mean') if 'awnd_ms' in w else ('tavg','mean')\n",
    "    ).rename(columns={'month':'date'})\n",
    "    dbg(f\"Weather monthly rows: {len(agg)}; span: {agg['date'].min()} → {agg['date'].max()}\")\n",
    "    return agg\n",
    "\n",
    "bus_monthly = aggregate_bus(bus)\n",
    "weather_monthly = aggregate_weather(weather)\n",
    "merged = (bus_monthly\n",
    "          .merge(weather_monthly, on='date', how='left', validate='1:1')\n",
    "          .merge(traffic_monthly, on='date', how='left')\n",
    "          .merge(alerts_df, on='date', how='left'))\n",
    "\n", 
    "dbg(f\"Merged shape: {merged.shape}\")\n",
    "dbg(f\"Merged span: {merged['date'].min()} → {merged['date'].max()}\")\n",
    "missing_report(merged, 'merged')\n",
    "\n",
    "if merged['mean_speed_mph'].isna().all():\n",
    "    warn(\"Traffic monthly is entirely NA — traffic timeframe might not overlap bus months. Consider increasing months_back_for_traffic.\")\n",
    "if merged['alert_count'].isna().any():\n",
    "    # keep NA to reflect true missingness; do not fill to avoid bias\n",
    "    warn(\"Alert counts contain NA for some months (single snapshot). Consider sampling alerts regularly and aggregating.\")\n",
    "\n",
    "display(merged.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📊 Visualizations\n",
    "At least four charts across multiple libraries (Seaborn, Matplotlib, Plotly). Guards prevent failures on sparse data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) WA Distribution (Seaborn)\n",
    "plt.figure()\n",
    "if 'wa_weighted' in merged and merged['wa_weighted'].notna().any():\n",
    "    sns.histplot(merged['wa_weighted'].dropna(), kde=True, bins=20)\n",
    "    plt.title('Distribution of Monthly Wait Assessment (Weighted)')\n",
    "    plt.xlabel('Wait Assessment (0–1)')\n",
    "    plt.ylabel('Frequency')\n",
    "else:\n",
    "    plt.text(0.1, 0.5, 'No WA data available for histogram', fontsize=12)\n",
    "    plt.title('Distribution of Monthly Wait Assessment — unavailable')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2) Correlation Heatmap (Seaborn)\n",
    "plt.figure()\n",
    "num_cols = [c for c in ['wa_weighted','tavg','prcp_mm','snow_mm','awnd_ms','mean_speed_mph','alert_count'] if c in merged.columns]\n",
    "corr_df = merged[num_cols].copy()\n",
    "if len(num_cols) >= 2 and corr_df.dropna().shape[0] >= 2:\n",
    "    sns.heatmap(corr_df.corr(), annot=True, cmap='coolwarm', fmt='.2f')\n",
    "    plt.title('Correlation: Weather, Traffic, Alerts vs Reliability')\n",
    "else:\n",
    "    plt.text(0.1, 0.5, 'Insufficient numeric data for correlation heatmap', fontsize=12)\n",
    "    plt.title('Correlation Heatmap — insufficient data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3) Traffic Speed vs WA (Matplotlib)\n",
    "plt.figure()\n",
    "if all(c in merged.columns for c in ['mean_speed_mph','wa_weighted']) and merged[['mean_speed_mph','wa_weighted']].dropna().shape[0] > 0:\n",
    "    plt.scatter(merged['mean_speed_mph'], merged['wa_weighted'])\n",
    "    plt.xlabel('Mean Traffic Speed (mph)')\n",
    "    plt.ylabel('Wait Assessment (0–1)')\n",
    "    plt.title('Bus Reliability vs Traffic Congestion')\n",
    "else:\n",
    "    plt.text(0.1, 0.5, 'Traffic or WA data missing for scatter', fontsize=12)\n",
    "    plt.title('Bus Reliability vs Traffic Congestion — insufficient data')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Interactive Line: WA and Traffic (Plotly)\n",
    "plot_cols = [c for c in ['wa_weighted','mean_speed_mph'] if c in merged]\n",
    "if plot_cols and merged['date'].notna().any():\n",
    "    fig = px.line(merged.sort_values('date'), x='date', y=plot_cols, title='Trends: Wait Assessment & Mean Traffic Speed')\n",
    "    fig.update_layout(xaxis_title='Date', yaxis_title='Value')\n",
    "    fig.show()\n",
    "else:\n",
    "    dbg('Skipping Plotly line chart; columns unavailable.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5) Interactive Scatter: Precipitation vs Alerts (Plotly)\n",
    "if all(c in merged.columns for c in ['prcp_mm','alert_count','wa_weighted']) and merged[['prcp_mm','alert_count']].dropna().shape[0] > 0:\n",
    "    fig2 = px.scatter(merged, x='prcp_mm', y='alert_count', color='wa_weighted',\n",
    "                      title='Precipitation vs Alert Count (color = WA)')\n",
    "    fig2.show()\n",
    "else:\n",
    "    dbg('Skipping Plotly scatter (precip vs alerts); insufficient data.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 🧠 Insights (Interim)\n",
    "- Distributions show central tendency and spread of WA.\n",
    "- Correlation heatmap highlights relationships between WA and environmental/operational variables.\n",
    "- Scatter plots provide directional checks for congestion and precipitation vs reliability.\n",
    "- Gaps are logged (e.g., alerts represent a snapshot unless polled periodically and aggregated)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧹 Data Cleaning & Transformation\n",
    "Model-ready table with explicit handling of missingness, duplicates, outliers, and dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_df = merged.copy()\n",
    "\n",
    "# Target must exist\n",
    "before = len(model_df)\n",
    "model_df = model_df.dropna(subset=['wa_weighted'])\n",
    "dbg(f\"Dropped {before - len(model_df)} rows without wa_weighted\")\n",
    "\n",
    "# De-duplicate by month\n",
    "dups = model_df.duplicated(subset=['date']).sum()\n",
    "if dups:\n",
    "    warn(f\"{dups} duplicate monthly rows found; keeping first occurrence\")\n",
    "    model_df = model_df.drop_duplicates(subset=['date'], keep='first')\n",
    "\n",
    "# IQR clipping for precipitation/snow\n",
    "for col in ['prcp_mm','snow_mm']:\n",
    "    if col in model_df:\n",
    "        q1, q3 = model_df[col].quantile([0.25, 0.75])\n",
    "        iqr = q3 - q1\n",
    "        lo, hi = q1 - 1.5*iqr, q3 + 1.5*iqr\n",
    "        n_lo = (model_df[col] < lo).sum(); n_hi = (model_df[col] > hi).sum()\n",
    "        if n_lo or n_hi:\n",
    "            warn(f\"Clipping {col} outliers: below={n_lo}, above={n_hi}\")\n",
    "            model_df[col] = model_df[col].clip(lo, hi)\n",
    "\n",
    "# Numeric enforcement\n",
    "num_cols = ['wa_weighted','pct_passing','tavg','tmax','tmin','prcp_mm','snow_mm','awnd_ms','mean_speed_mph','alert_count']\n",
    "for c in num_cols:\n",
    "    if c in model_df: model_df[c] = pd.to_numeric(model_df[c], errors='coerce')\n",
    "\n",
    "missing_report(model_df, 'model_df')\n",
    "dbg(f\"Model DF: {model_df.shape}\")\n",
    "display(model_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning Summary\n",
    "- **Missing values:** Rows without `wa_weighted` dropped; remaining NA retained to avoid artificial signal.\n",
    "- **Duplicates:** Removed by month key.\n",
    "- **Outliers:** IQR clipping for `prcp_mm` and `snow_mm`.\n",
    "- **Types:** Numeric enforcement across all modeling columns; monthly `date` index aligned for joins.\n",
    "- **Sources:** CSV (bus, weather), JSON API (traffic), Protocol Buffers API (alerts)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ✅ Rubric Coverage Checklist\n",
    "- **EDA**: summaries, distributions, correlations, issues, dtype checks.\n",
    "- **Visualizations**: ≥4 charts across Seaborn, Matplotlib, Plotly (with descriptions).\n",
    "- **Cleaning**: missing, duplicates, outliers, types with justifications.\n",
    "- **Three distinct I/O methods**: CSV, JSON API, Protocol Buffers API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🧩 Machine Learning Plan (Preview)\n",
    "- **Target:** `wa_weighted`\n",
    "- **Features:** `tavg`, `prcp_mm`, `snow_mm`, `awnd_ms`, `mean_speed_mph`, `alert_count`\n",
    "- **Models:** Linear Regression → Ridge/Lasso → Random Forest\n",
    "- **Evaluation:** Time-aware split (e.g., train 2020–2023, test 2024); metrics: MAE, RMSE, R².\n",
    "- **Challenges:** Limited alert variability unless alerts are sampled regularly; timeframe alignment between sources; potential multicollinearity (address via regularization)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🔄 Prior Feedback & Updates\n",
    "- Switched from embedded payloads to **live API calls** (traffic JSON + alerts protobuf).\n",
    "- Added retry/backoff, strict SSL, and key-based auth handling.\n",
    "- Expanded logs, sanity checks, and defensively coded visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep this as the last cell per assignment instructions -------------------\n",
    "!jupyter nbconvert --to python source.ipynb"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "IT4063C",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
